{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_toy_example.py\n",
    "import math, random, torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# ===== 1) Dữ liệu toy =====\n",
    "# Ta map a,b,c,d,e -> x,y,z,u,v (dịch đơn giản)\n",
    "src_vocab = {'<pad>':0, '<sos>':1, '<eos>':2, 'a':3, 'b':4, 'c':5, 'd':6, 'e':7}\n",
    "tgt_vocab = {'<pad>':0, '<sos>':1, '<eos>':2, 'x':3, 'y':4, 'z':5, 'u':6, 'v':7}\n",
    "inv_tgt = {v:k for k,v in tgt_vocab.items()}\n",
    "\n",
    "def make_sample():\n",
    "    seq = random.choices(['a','b','c','d','e'], k=random.randint(3,5))\n",
    "    trg = [chr(ord(ch)+23) for ch in seq]  # a->x, b->y, c->z, d->u, e->v\n",
    "    return seq, trg\n",
    "\n",
    "def encode(seq, vocab):\n",
    "    return [vocab['<sos>']] + [vocab[s] for s in seq] + [vocab['<eos>']]\n",
    "\n",
    "def pad_seq(seq, max_len):\n",
    "    return seq + [0]*(max_len - len(seq))\n",
    "\n",
    "# Tạo batch toy\n",
    "def gen_batch(batch_size=4):\n",
    "    srcs, tgts = [], []\n",
    "    for _ in range(batch_size):\n",
    "        s, t = make_sample()\n",
    "        srcs.append(encode(s, src_vocab))\n",
    "        tgts.append(encode(t, tgt_vocab))\n",
    "    max_s, max_t = max(len(s) for s in srcs), max(len(t) for t in tgts)\n",
    "    src_pad = torch.tensor([pad_seq(s, max_s) for s in srcs], device=DEVICE)\n",
    "    tgt_pad = torch.tensor([pad_seq(t, max_t) for t in tgts], device=DEVICE)\n",
    "    return src_pad, tgt_pad\n",
    "\n",
    "# ===== 2) Mô hình =====\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # [1,max_len,d_model]\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "class TransformerToy(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab, d_model=64, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.src_emb = nn.Embedding(len(src_vocab), d_model)\n",
    "        self.tgt_emb = nn.Embedding(len(tgt_vocab), d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=128,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, len(tgt_vocab))\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src,tgt: [B, T]\n",
    "        src_mask = self.make_pad_mask(src)\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(DEVICE)\n",
    "        src_emb = self.pos_enc(self.src_emb(src))\n",
    "        tgt_emb = self.pos_enc(self.tgt_emb(tgt))\n",
    "        out = self.transformer(\n",
    "            src_emb, tgt_emb,\n",
    "            src_key_padding_mask=src_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=self.make_pad_mask(tgt)\n",
    "        )\n",
    "        return self.fc_out(out)\n",
    "\n",
    "    def make_pad_mask(self, x):\n",
    "        # True tại vị trí PAD\n",
    "        return (x == 0)\n",
    "\n",
    "# ===== 3) Huấn luyện nhanh =====\n",
    "model = TransformerToy(src_vocab, tgt_vocab).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 300\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    src, tgt = gen_batch(batch_size=16)\n",
    "    optimizer.zero_grad()\n",
    "    out = model(src, tgt[:, :-1])  # predict bước kế từ <sos>...<n-1>\n",
    "    loss = criterion(out.reshape(-1, len(tgt_vocab)), tgt[:, 1:].reshape(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if ep % 50 == 0 or ep == 1:\n",
    "        print(f\"[Epoch {ep}] loss={loss.item():.4f}\")\n",
    "\n",
    "# ===== 4) Suy luận =====\n",
    "@torch.no_grad()\n",
    "def translate(seq):\n",
    "    model.eval()\n",
    "    src = torch.tensor([encode(seq, src_vocab)], device=DEVICE)\n",
    "    tgt = torch.tensor([[tgt_vocab['<sos>']]], device=DEVICE)\n",
    "    for _ in range(10):\n",
    "        out = model(src, tgt)\n",
    "        next_tok = out[:, -1, :].argmax(-1).unsqueeze(1)\n",
    "        tgt = torch.cat([tgt, next_tok], dim=1)\n",
    "        if next_tok.item() == tgt_vocab['<eos>']:\n",
    "            break\n",
    "    pred = [inv_tgt[i.item()] for i in tgt[0][1:-1]]\n",
    "    return \"\".join(pred)\n",
    "\n",
    "# Thử một vài chuỗi\n",
    "for s in [[\"a\",\"b\",\"c\"], [\"d\",\"e\"], [\"a\",\"c\",\"e\",\"b\"]]:\n",
    "    print(f\"{s} -> {translate(s)}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
